% !TEX root = ../main.tex

\chapter{Machine Learning (D. Marazzina)}

\begin{itemize}
    \item Machine learning is a branch of AI.
    \item The idea underlying machine learning is that we give a computer program access to lots of data and let it learn about relationships between variables and make predictions.
    \item Some of the techniques of machine learning date back to the 1950s but improvements in computer speeds and data storage costs have now made machine learning a practical tool.
\end{itemize}

Different kinds of \textit{learning:}
\begin{itemize}
    \item \textbf{Unsupervised} learning (find patterns)
          \begin{itemize}
              \item Examples: interpolation (with noise data), calibration, lapse prediction 
              \item Methods: Neural Networks, Decision Trees, Random Forests
          \end{itemize}
    \item \textbf{Supervised} learning (predict numerical value or classification)
          \begin{itemize}
              \item Examples: credit scoring, client selection
              \item Methods: Networks, Clustering
          \end{itemize}
    \item \textbf{Reinforcement learning} (multi-stage decision making)
          \begin{itemize}
              \item Examples: trading
              \item Methods: Q-learning and Neural Network/Random Forest (to approximate matrix $Q$)
          \end{itemize}
\end{itemize}

Software:
\begin{itemize}
    \item There a several alternatives such as Python, R, MATLAB, Spark, and Julia.
    \item Need ability to handle very large data sets and availability of packages that implement the algorithms.
    \item Python seems to be winning at the moment.
\end{itemize}

\paragraph{Traditional statistics} Means, SDs, Probability distributions, Significance tests, Confidence intervals, Linear regression.

\paragraph{Modern statistics}
\begin{itemize}
    \item Huge data sets 
    \item Fantastic improvements in computer processing speeds and data storage costs
    \item Machine learning tools are now feasible.
    \item Can now develop non-linear prediction models, find patterns in data in ways that were not possible before, and develop multi-stage decision strategies
    \item New terminology: features, labels, activation functions, target, bias, supervised/unsupervised learning.
\end{itemize}

Features are the \textit{old} independent variables for linear regression, but in ML it is no more important that they are independent (no problem of collinearity).

ML good practice:
\begin{itemize}
    \item Divide data into three sets
          \begin{itemize}
              \item Training set
              \item Validation set
              \item Test set
          \end{itemize}
    \item Develop different models using the training set and compare them using the validation set
    \item Rule of thumb: increase model complexity until model no longer generalizes well to the validation set
    \item The test set is used to provide a final out-of-sample indication of how well the chosen model works
\end{itemize}

\section{Data cleaning}

Data cleaning refers to identifying and correcting errors in the dataset that may negatively impact a predictive model. Data cleaning is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Although critically important, data cleaning is not exciting, not does it involve fancy techniques. Just a good knowledge of the dataset. Cleaning up your data is not the most glamorous of tasks, but it's an essential part of data wrangling. Knowing how to properly clean and assemble your data will set you miles apart from others in your field. \textit{Garbage in, garbage out!}

There are many types of errors that exist in a dataset, although some of the simplest errors include columns that don't contain much information and duplicated rows.
\begin{itemize}
    \item Identify (and delete) columns that contain a single value
    \item Consider columns that have very few values (near $0$ variance)
    \item Identify (and delete) rows that contain duplicate data
\end{itemize}

It is not always true that more features you gave, better it is. If you think that a feature is not important for your model, try to remove it.

If some observations contain missing or inconsistent data, consider the possibility
\begin{itemize}
    \item to remove the observation
    \item to remove the feature (if this affect only a feature and a large number of observations, e.g., only 60\% of observations has the data of a given feature known)
    \item to “fill the gap” inserting, e.g., a mean/median value (if numerical) or interpolating (if they are time-dependent data, and you know the values at time $t_{i-1}$ and $t_{i+1}$, but not at time $t_{i}$).
\end{itemize}

\textbf{Isolation forest} is an algorithm for anomaly detection that works on the principle of isolating anomalies. In statistics, an \textbf{anomaly} (or \textbf{outlier}) is an observation or event that deviates so much from other events to arouse suspicion it was generated erroneously.

Anomalies in a big dataset may follow very complicated patterns, which are difficult to detect “by eye” in the great majority of cases. Isolation Forest explicitly isolates anomalous points in the dataset. From a mathematical point of view, recursive partitioning can be represented by a tree structure named Isolation Tree, while the number of partitions required to isolate a point can be interpreted as the length of the path, within the tree, to reach a terminating node starting from the root. Intuitively, the anomalous points are those (easier to isolate, hence) with the smaller path length in the tree.

\section{Unsupervised Learning}

\begin{itemize}
    \item In unsupervised learning we are not trying to predict anything.
    \item The objective is to \textbf{cluster} data to increase our \textbf{understanding} of the environment.
    \item How? With the \textbf{$k$-means algorithm}.
\end{itemize}

Before using many ML algorithms (including those for unsupervised learning), it is important to scale feature values so that they are comparable.
\begin{itemize}
    \item \textbf{Z-score} scaling involves calculating the mean $m$ and $SD$ from the values of each feature from the training set. Scaled feature values for all data sets are then created by subtracting the mean and dividing by the $SD$. The scaled feature values have a $m=0$ and $SD=1$
          \begin{equation*}
              \frac{V-m}{SD}
          \end{equation*}
    \item An alternative is the \textbf{Min-max} scaling: it involves calculating the maximum and minimum value of each feature from the training set. Scaled feature values for all data sets are then created by subtracting the minimum and dividing by the difference between the maximum and minimum. The scaled feature values lie between zero and one.
          \begin{equation*}
              \frac{V-\min}{\max -\min}
          \end{equation*}
\end{itemize}

For clustering we need a \textbf{distance} measure. The simplest distance measure is the Euclidean distance measure.
\begin{equation*}
    d\left( A,B\right) =\sqrt{\left( x_{B} -x_{A}\right)^{2} +\left( y_{B} -y_{A}\right)^{2}}
\end{equation*}
In general when there are $m$ features the distance between $P$ and $Q$ is
\begin{equation*}
    d\left( P,Q\right) =\sqrt{\sum\limits _{j=1}^{m}\left( v_{pj} -v_{qj}\right)^{2}}
\end{equation*}
where $v_{pj} ,v_{qj}$ are the values of the $j$-th feature for $P$ and $Q$.

Now that we have a distance measure, it is important to define the \textbf{center of a cluster}, also known as \textbf{cluster centroid}.

\textbf{$k$-means algorithm to find $k$ clusters:}
\begin{enumerate}
    \item Choose $k$ random points as cluster centers
    \item Assign each observation no the nearest center
    \item Calculate new cluster centers
    \item If cluster centers have changed, go to (2) otherwise end.
\end{enumerate}

\textbf{Inertia:}

For any given $k$ the objective is to minimize \textbf{inertia}, which is defined as the within cluster sum of squares:
\begin{equation*}
    \text{inertia} =\sum _{i=1}^{n} d{_{i}}^{2}
\end{equation*}
where $d_{i}$ is the distance of observation $i$ from its cluster center, and $n$ is the number of observations. In practice we use the $k$-means algorithm with several different starting points and choose the result that has the smallest inertia.

Generally, \textbf{the inertia decreases as }$k$\textbf{ increases}, approaching $0$ when $k=n$, i.e., we have $n$ observations and $n$ clusters, i.e., each cluster coincide with a single observation.

\textbf{Therefore we need to find a proper way to set }$k$.

\begin{itemize}
    \item \textbf{The elbow approach:} in this case, $k=4$ seems the best choice, since the inertia decreases of a small quantity moving from $4$ to $5$ w.r.t from $3$ to $4$.
    \item \textbf{The elbow approach + distance between clusters analysis:} if moving from $k$ to $k+1$ clusters we create a cluster with center very close to the center of another cluster, we stop the algorithm to $k$.
    \item \textbf{The silhouette method:} for each observation $i$ calculate $a( i)$, the average distance from other observations in its cluster, and $b( i)$, the average distance from observations in the \textbf{closest other cluster}, i.e., for each cluster (excluding the one which contains the observation), we compute the average distance of the considered observation $i$ from the observations of the cluster, and \textbf{then take the minimum}. The silhouette score for observation $i$, $s( i)$, is defined as
          \begin{equation*}
              s( i) =\frac{b( i) -a( i)}{\max\{a( i) ,b( i)\}}
          \end{equation*}
          
          Choose the number of clusters that maximizes the average silhouette score across all observations.
          The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from $-1$ to $+1$, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.
    \item The \textbf{gap statistic} compares the within cluster sum of squares with what would be expected with random data.
          \begin{itemize}
              \item Let us create $N$ sets of random points.
              \item For each set, let us cluster this points in $k$ clusters, and compute the Inertia, for different $k$.
              \item Let us define with
                    \begin{itemize}
                        \item $w_{k}$ the inertia computed for the real data
                        \item $m_{k}( s_{k})$ the mean value (standard deviation) of the $N$ inertias computed exploiting the random sets
                    \end{itemize}
              \item We define the Gap as
                    \begin{equation*}
                        Gap( k) =m_{k} -w_{k}
                    \end{equation*}
              \item We set $k$ such that
                    \begin{equation*}
                        Gap( k) \in ( s_{k+1} ,Gap( k+1))
                    \end{equation*}
          \end{itemize}
\end{itemize}

\subsection{The Curse of Dimensionality}

\begin{itemize}
    \item The Euclidean distance measure increases as the number of features increase.
    \item This is referred to as the \textbf{curse of dimensionality}, and could prevent the $k$-means algorithm to work efficiently.
    \item Consider two observations that have values for feature $j$ equal to $x_{j}$ and $y_{j}$. An alternative distance measure that always lies between $0$ and $2$ is
          \begin{equation*}
              1-\frac{\sum _{j=1}^{m} x_{j} y_{j}}{\sqrt{\sum _{j=1}^{m} x_{j}^{2}\sum _{j=1}^{m} y_{j}^{2}}}
          \end{equation*}
\end{itemize}

Now we see some alternative clustering approaches.

\subsection{Hierarchical Clustering}

\begin{itemize}
    \item Start with each observation in its own cluster
    \item Combine the two closest clusters
    \item Continue until all observations have been combined into a single cluster (or since we have only $k$ clusters)
    \item Can be implemented in Python with \texttt{sklearn.cluster.AgglomerativeClustering}.
    \item Measures of closeness of clusters:
          \begin{itemize}
              \item Average Euclidean distance between points in clusters
              \item Maximum distance between points in clusters
              \item Minimum distance between points in clusters
              \item Increase in inertia (a version of Ward's method)
          \end{itemize}
\end{itemize}

\subsection{Density-based clustering}

\begin{itemize}
    \item Forms clusters based on the closeness of individual observations.
    \item Unlike $k$-means the algorithm, it is not based on cluster centers.
    \item We might initially choose 8 observations that are close. After that we add an observation to the cluster if it is close to at least 5 other observations in the cluster, and repeat. 
\end{itemize}

\subsection{Distribution-based Clustering}

Assumes that observations come from a mixture of distributions and uses statistical procedures to separate the distributions.

\section{Reduce dimensionality}

\subsection{Principal Components Analysis (PCA)}

This is an approach to reduce the number of variables
\begin{itemize}
    \item PCA replaces a set of $n$ variables by $n$ \textbf{new} factors so that:
          \begin{itemize}
              \item Any observation on the original variables is a linear combination of the $n$ factors
              \item The $n$ factors are uncorrelated
              \item The quantity of a particular factor in a particular observation is the factor score
              \item The importance of a particular factor is measured by the standard deviation of its factor score across observations \ 
          \end{itemize}
    \item The idea is to find a few variables that \textbf{account for a high percentage of the variance} in the observations.
\end{itemize}

The algorithm goes as follow:
\begin{enumerate}
    \item Standardize data with the Z-score scaling.
    \item Compute the variance-covariance matrix.
    \item Compute its eigenvalues and eigenvectors.
    \item Eigenvectors are the Principal Components. \textit{Higher is the eigenvalues, more important is the corresponding principal components}.
    \item The factor scores are the eigenvalues, and the SD of the factor scores are the square root of the eigenvalues itself.
\end{enumerate}

Last step: \textbf{recast the data along the principal components axes}.

In the previous steps, apart from standardization, we did not make any changes on the data, we just selected the principal components and formed the feature vector, but the input data set remains always in terms of the original axes (i.e, in terms of the initial variables).

We now need to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.
\begin{equation*}
    \text{FinalDataSet} =\text{FeatureVector}\transpose \cdot \text{StandardizedOriginalDataSet}\transpose
\end{equation*}

\section{Supervised Learning: with focus on Classification}

\subsection{Linear Regression}

Linear regression is a very popular tool because once you have made the assumption that the model is linear you do not need huge amount of data.

In ML we refer to the constant term as the \textit{bias} and the coefficients as \textit{weights}. Assume $m$ features and $n$ observations:
\begin{equation*}
    Y=a+b_{1} X_{1} +\cdots +b_{m} X_{m} +\varepsilon ,
\end{equation*}
we choose $a,b_{i}$ to minimize the mean squared error:
\begin{equation*}
    MSE=\frac{1}{n}\sum _{j=1}^{n}[ Y_{j} -( a+b_{1} X_{1,j} +\cdots +b_{m} X_{m,j})]^{2}
\end{equation*}
This can be done analytically by inverting a matrix. Alternatively a numerical (\textbf{gradient descent}) method can be used. The objective is to minimize a function by changing parameters. Steps are as follows:
\begin{enumerate}
    \item Choose starting value for parameters.
    \item Find the steepest slope: i.e. the direction in which parameter have to be changed to reduce the objective function by the greatest amount.
    \item Take a step down the valley in the direction of the steepest slope.
    \item Repeat steps (2) and (3).
    \item Continue until you reach the bottom of the valley.
\end{enumerate}

The $p$-value for each term tests the null hypothesis that the coefficient is equal to zero (no effect). \textbf{A low }$p$\textbf{-value (less than 0.05) indicates that you can reject the null hypothesis}. In other words, a predictor that has a low $p$-value is likely to be a meaningful addition to your model because changes in the predictor's value are related to changes in the response variable.

$R$-square varies between 0 and 1: when 0, the model used does not explain the data at all; when it is 1 the model perfectly explains (i.e \textit{describes}, bad!) the data. 

\textbf{Categorical features} are features where there are a number of \textit{non-numerical} alternatives. We can define a \textit{dummy variable} for each alternative. The variable equals 1 if the alternative is true and zero otherwise. This is known as one-hot encoding. But sometimes we do not have to do this because there is a natural ordering of variables, e.g.:
\begin{itemize}
    \item small 1, medium 2, large 3.
    \item assist. prof 1, assoc. prof 2, full prof 3.
\end{itemize}

\textbf{Dummy Variably Trap.} Suppose we have a constant term and a number of dummy variables (equal to 0 or 1) where, for each observation, only one dummy can take value 1 (e.g., dummies $X_{1} ,X_{2} ,\dotsc $: dummy \textit{born in Europe, born in Africa}, ...).

There is then no unique solution because, for any $C$, we can add $C$ to the constant term and subtract $C$ from each of the dummy variables coefficient \textbf{without changing the prediction.}
\begin{equation*}
    Y\ =\ a+b_{1} X_{1} +b_{2} X_{2} =a+C+( b_{1} -C) X_{1} +( b_{2} -C) X_{2}
\end{equation*}
\textbf{Regularization} solves this problem.
\begin{itemize}
    \item Linear regression can over-fit, particularly when there are a large number of correlated features.
    \item Results for validation set may not then be as good as for training set
    \item Regularization is a way of avoiding over fitting and reducing the number of features. Possible Regularization technique:
          \begin{itemize}
              \item Ridge 
              \item Lasso
              \item Elastic net
          \end{itemize}
    \item We must first scale feature values
\end{itemize}

\subsection{Ridge regression}

\begin{itemize}
    \item Reduce magnitude of regression coefficients by choosing a parameter $\lambda $ and minimizing:
          
          \begin{equation*}
              \frac{1}{n}\sum\limits _{j=1}^{n}\left[ Y_{j} -\left( a+b_{1} X_{1,j} +\cdots +b_{m} X_{m,j}\right)\right]^{2} +\lambda \sum _{i=1}^{m} b_{i}^{2}
          \end{equation*}
    \item What happens as $\lambda $ increases? Minimizing the above function, all $b_{i}$s approach 0 as $\lambda $ increases.
    \item Ridge regression, as linear regression, can be solved analytically.
\end{itemize}

\subsection{Lasso Regression}

\begin{itemize}
    \item Similar to ridge regression except we minimize:
          
          \begin{equation*}
              \frac{1}{n}\sum\limits _{j=1}^{n}\left[ Y_{j} -\left( a+b_{1} X_{1,j} +\cdots +b_{m} X_{m,j}\right)\right]^{2} +\lambda \sum _{i=1}^{m}\left| b_{i}\right| 
          \end{equation*}
    \item This has the effect of completely eliminating the less important factors .
    \item We must use gradient descent.
\end{itemize}

\subsection{Elastic Net Regression}

\begin{itemize}
    \item Middle ground between \textit{Ridge} and \textit{Lasso:}
    \item Minimize:
          \begin{equation*}
              \frac{1}{n}\sum\limits _{j=1}^{n}[ Y_{j} -( a+b_{1} X_{1,j} +\cdots +b_{m} X_{m,j})]^{2} +\lambda _{1}\sum _{i=1}^{m} b_{i}^{2} +\lambda _{2}\sum _{i=1}^{m}| b_{i}| 
          \end{equation*}
    \item Must use gradient descent.
\end{itemize}

\section{Logistic Regression}

\begin{itemize}
    \item The objective is to classify observations into a “positive outcome” and “negative outcome” using data on features
    \item Dependent variable: binary (1 – 0)
    \item Probability of a positive outcome is assumed to be a \textbf{sigmoid} function:
          \begin{equation*}
              Q=\frac{1}{1+e^{-Y}}
          \end{equation*}where $Y$ is related linearly to the values of the features:
          \begin{equation*}
              Y=a+b_{1} X_{1} +\cdots +b_{m} X_{m}
          \end{equation*}          
\end{itemize}

\fg{0.7}{b73959a12b4ee61d6858da9d987c1ffa-I7e1tfoeOe-image.png}

$Y\in \{0,1\}$. Let $Q=p\left( X\right)$ be the probability that $Y=1$, conditioned to the value of $X$.
\begin{equation*}
    \log\left(\frac{p\left( X\right)}{1-p\left( X\right)}\right) =\beta _{0} +\beta _{1} X
\end{equation*}
Multivariate
\begin{equation*}
    \log\left(\frac{p( X)}{1-p( X)}\right) =\beta _{0} +\beta _{1} X_{1} +\cdots +\beta _{k} X_{k}
\end{equation*}
\textbf{Maximum Likelihood Estimation (MLE)}
\begin{itemize}
    \item We use the training set to maximize
          \begin{equation*}
              \sum _{\text{pos. outcomes}}\log( Q) +\sum _{\text{neg. outcomes}}\log( 1-Q)
          \end{equation*}
    \item This cannot be maximized analytically but we can use a gradient ascent algorithm.
\end{itemize}

\textbf{Logistic regression as classifier.}

Once betas are estimated, we use the regression to estimate $p\left( X\right)$.

We have to choose a \textbf{threshold!}
\begin{gather*}
    p >\overline{Z} \ \ \Longrightarrow \ \ \hat{Y} =1\\
    p< \overline{Z} \ \ \Longrightarrow \ \ \hat{Y} =0
\end{gather*}

\begin{itemize}
    \item \textbf{Sensitivity.} True Positive fraction, the proportion of class 1 (positive) members classified as such.
    \item \textbf{Specificity.} True Negative fraction, the proportion between the members of class 0 (negative) classified as such.
\end{itemize}

The Confusion matrix and common ratios

\begin{center}
    
    \begin{tabular}{|c|c|c|}
        \hline 
                     & Predict pos. & Predict neg. \\
        \hline 
        Outcome pos. & $TP$         & $FN$         \\
        \hline 
        Outcome neg. & $FP$         & $TN$         \\
        \hline
    \end{tabular}
\end{center}

\begin{gather*}
    \text{accuracy} =\frac{TP+TN}{\text{all}}\\
    \text{True Positive Ratio (TPR or sensitivity)} =\frac{TP}{TP+FN}\\
    \\
    \text{True Negative Ratio (specificity)} =\frac{TN}{TN+FP}\\
    \\
    \text{False Positive Ratio} =\frac{FP}{TP+FP}\\
    \\
    \text{Precision} ,P=\frac{TP}{TP+FP}\\
    \\
    F\text{-score} =2\cdot \frac{P\cdot TPR}{P+TPR}
\end{gather*}
\textbf{ROC Curve (receiver operating characteristic).}

Sensitivity and specificity are tested by varying the classifier parameters - e.g. the logistic classifier barrier.

\fg{0.7}{bdab95cc33820afd21b5c1ad51bbcb05-fkO7SSo4fY-image.png}

The performance of a classifier can be measured using the so-called \textbf{AUC, the area under the ROC curve}. \ The \textit{greater the area the greater the goodness} (sensitivity and specificity) that the classifier is able to obtain. The bisector in the graph is obtained through a trivial classifier (I randomly assign the observations to the two classes). For a classifier to be useful, the \textbf{AUC must beat the trivial classifier} (i.e. AUC greater than 0.5).

The area under the curve is a popular way of summarizing the predictive ability of a model to estimate a binary variable
\begin{itemize}
    \item When AUC = 1 the model is perfect. 
    \item When AUC = 0.5 the model has no predictive ability.
    \item When AUC < 0.5 the model is worse than random.
\end{itemize}

\textbf{Imbalanced Dataset} typically refers to a problem with classification problems where the classes are not represented equally.

The \textit{accuracy paradox} is the name for the exact situation in the introduction to this post.

It is the case where your accuracy measures tell the story that you have excellent accuracy (such as 90\%), but the accuracy is only reflecting the underlying class distribution.

If we have 99\% of observations belonging to class 1, the trivial predictor giving as output «class 1» for any input will have a 99\% accuracy.

\begin{itemize}
    \item \textbf{Can You Collect More Data?}
    \item \textbf{Try Changing Your Performance Metric}
          
          Accuracy is not the metric to use when working with an imbalanced dataset. We have seen that it is misleading.
    \item \textbf{Try Resampling Your Dataset}
          
          You can change the dataset that you use to build your predictive model to have more balanced data.
          
          This change is called sampling your dataset and there are two main methods that you can use to even-up the classes:
          
          You can add copies of instances from the under-represented class called over-sampling (or more formally sampling with replacement), or
          
          You can (randomly) delete instances from the over-represented class, called under-sampling.
    \item \textbf{Try Generate Synthetic Samples}
          
          A simple way to generate synthetic samples is to randomly sample the attributes from instances in the minority class.
    \item \textbf{Try Penalized Models}
          
          You can use the same algorithms but give them a different perspective on the problem.
          
          Penalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class.
\end{itemize}

\subsection{Alternative to regression: clusterization.}

Use Unsupervised Learning to construct clusters.
\begin{itemize}
    \item Each cluster corresponds to a set of probabilities, e.g., the probability of being a class 1 cluster is equal to the number of observations in this cluster belonging to class 1 divided by the total number of observations in this cluster.
    \item If a new observation fall in a cluster, it inherits all the cluster's class probabilities.
\end{itemize}

\subsection{Alternative to regression: $k$-nearest neighbors}

\begin{itemize}
    \item Normalize data.
    \item Measure the distance in $n$-dimensional space of the new data from the data for which there are labels (i.e. known outcomes).
    \item Distance of point with feature values $x_{i}$ from point with feature values $y_{i}$ is the euclidean distance.
    \item Choose the $k$ closest data items and average their labels.
    \item For example if you are forecasting car sales in a certain area with $k=3$ and the three nearest neighbors for GDP growth and interest rates give sales of 5.2, 5.4 and 5.6 million units, the forecast would be the average of these or 5.4 million units.
    \item If you are forecasting whether a loan will default with $k=5$ and that of the five nearest neighbors four defaulted and one was good loan, you would estimate an 80\% chance of default
\end{itemize}

\subsection{Networks for clustering}

Graph (directed/undirected/partially directed/weighted/bipartite).

Adjacency matrix
\begin{equation*}
    A_{ij} =
    \begin{cases}
        1 & i,j\ \text{connected} \\
        0 & \text{otherwise}      
    \end{cases}
\end{equation*}
Adjacency list: stores only the non zero elements of the Adjacency matrix in a list of all vertices.

Centrality measures: \textit{which vertices are more important?}
\begin{itemize}
    \item \textbf{degree centrality,}
    \item \textbf{closeness centrality,}
    \item \textbf{betweenness centrality,}
    \item \textbf{eigenvector centrality.}
\end{itemize}

\textbf{Degree Centrality}
\begin{itemize}
    \item The simplest measure of importance is the degree centrality
    \item The vertex with largest degree exerts the greatest effect on the network
    \item Degree centrality: number of nearest neighbours
          
          \begin{equation*}
              \boxed{D(i)=\sum _{j=1}^{n} A_{ij} =\sum _{j=1}^{n} A_{ji}}
          \end{equation*}
    \item Sensitive to the addition of one more node
    \item We cannot compare degree centrality of vertices belonging to \textit{two different} networks
    \item Normalized degree centrality (relative metric to compare degree centrality)
          
          \begin{equation*}
              \boxed{N_{D} (i)=\frac{1}{n-1} D(i)}
          \end{equation*}
\end{itemize}

In directed graphs:
\begin{itemize}
    \item \textbf{In-Degree:} the number of edges \textit{incoming} to a vertex
          
          \begin{equation*}
              d_{i}^{\leftarrow } =\sum _{j=1}^{n} A_{ji}
          \end{equation*}
    \item \textbf{Out-Degree:} the number of edges \textit{outgoing} from a vertex
          
          \begin{equation*}
              d_{i}^{\rightarrow } =\sum _{j=1}^{n} A_{ij}
          \end{equation*}
\end{itemize}

\textbf{Shortest Path}

A \textbf{path} in a network is a sequence of vertices $x,y,\dotsc ,z$ such that each consecutive pair of vertices $i,j$ is connected by an edge $(i,j)$ in the network.

The \textbf{shortest path} is the shortest of all possible paths between two vertices.

\textbf{Average Path Length}

The average graph-distance between all pair of nodes
\begin{equation*}
    L_{G} =\frac{1}{n(n-1)}\sum _{i\neq j} d( v_{i} ,v_{j})
\end{equation*}
If all nodes are connected, then the graph distance equals to 1

\textbf{Closeness Centrality}
\begin{itemize}
    \item Measures how close a node is to all the other nodes in network (in terms of the shortest path)
          
          \begin{equation*}
              \boxed{C(i)=\frac{1}{\sum _{j=1}^{n} d(i,j)}}
          \end{equation*}
          
          where $d(i,j)$ is the \textit{shortest} path between the nodes $i$ and $j$.
    \item Normalized closeness centrality (relative metric to compare closeness centrality)
          
          \begin{equation*}
              \boxed{N_{C} (i)=(n-1)C(i)}
          \end{equation*}
\end{itemize}

\textbf{Betweenness Centrality}
\begin{itemize}
    \item The number of shortest paths from all vertices to all others that pass through a node.
          
          \begin{equation*}
              \boxed{B(i)=\sum _{s\neq i\neq t}\frac{\delta _{st} (i)}{\delta _{st}}}
          \end{equation*}
          
          where $st\left( i\right)$ is the number of shortest paths between $s$ and $t$ that pass through $i$, and $st$ is the total number of shortest paths between $s$ and $t$.
    \item Normalized betweenness centrality
          
          \begin{equation*}
              N_{B} (i)=\frac{2}{(n-1)(n-2)} B(i)
          \end{equation*}
    \item Probability that a communication from $s$ to $t$ will go through $i$.
\end{itemize}

\textbf{Eigenvector Centrality}
\begin{itemize}
    \item Eigenvector centrality measures assign an importance score to vertices in a way that is proportional to the importance scores of its neighbors hence the importance of a node depends on the importance of its neighbors.
    \item This involves an eigenvector problem of the form
          
          \begin{equation*}
              Av=\lambda _{1} v
          \end{equation*}
          
          where $A$ is the adjacency matrix, $v$ is a vector containing the eigenvector centralities, and $\lambda _{1}$ is the \textit{largest eigenvalue} of $A$.
    \item Other forms of eigenvector centrality
          \begin{itemize}
              \item Bonacichs centrality
              \item PageRank centrality
              \item Katz centrality and hub/authority scores
          \end{itemize}
\end{itemize}

\fg{0.7}{1d2e322c742378851516d74d8279ab20-raPewaxZmA-image.png}

Consider

\fg{0.7}{a493da630bcacc23f216f97ca34bbe1c-sf8t5DGE6D-image.png}

Comparison of different centralities:
\begin{itemize}
    \item Degree most Central Nodes: D and K.
    \item Closeness most Central Nodes: H.
    \item Betweenness most Central Nodes: H.
    \item Eigenvector most Central Nodes: D and K.
\end{itemize}

\section{Decision Trees}

\subsection{Measures of Uncertainty}

\begin{itemize}
    \item Suppose that there are $n$ possible outcomes and $p_{i}$ is the probability of outcome $i$ with $\sum _{i} p_{i} =1$.
    \item \textbf{Entropy} measure of uncertainty:
          \begin{equation*}
              \text{Entropy} =\mathbb{E}\left[\text{information}\right] =\sum _{i=1}^{n} p_{i} \cdot \log\left(\frac{1}{p_{i}}\right) =-\sum _{i=1}^{n} p_{i}\log p_{i}
          \end{equation*}
    \item \textbf{Gini} Measure of uncertainty:
          \begin{equation*}
              \text{Gini} =1-\sum _{i=1}^{n} p_{i}^{2}
          \end{equation*}
    \item In case of a binary variable ($n=2$) we have
          \begin{equation*}
              \text{Gini} =1-\left( p^{2} +\left( 1-p\right)^{2}\right) =2p\left( 1-p\right)
          \end{equation*}
\end{itemize}

\subsection{Information Gain}

\begin{itemize}
    \item The information gain is the expected decrease in uncertainty (as measured by either entropy or Gini).
    \item Suppose that there is a 20\% chance that a person will receive a job offer
    \item Suppose further that there is a 50\% chance the person has a relevant degree. If the person does have a relevant degree the probability of a job offer rises to 30\%, otherwise it falls to 10\%
    \item Initial entropy:
          \begin{equation*}
              -[ 0.2\log( 0.2) +0.8\log( 0.8)] =0.7219
          \end{equation*}
    \item Expected entropy:
          \begin{equation*}
              -0.5\left[ 0.1\log\left( 0.1\right) +0.9\log\left( 0.9\right)\right] -0.5\left[ 0.3\log\left( 0.3\right) +0.7\log\left( 0.7\right)\right] =0.6751\ \ \ \ 
          \end{equation*}
    \item Expected information gain from knowing whether there is a relevant degree:
          \begin{equation*}
              0.7219-0.6751=0.0468
          \end{equation*}
\end{itemize}

\subsection{The Decision Tree Algorithm}

\begin{itemize}
    \item Algorithm chooses the feature at the root of the tree that has the greatest expected information gain.
    \item At subsequent nodes it chooses the feature (not already chosen) that has the greatest expected information gain.
    \item When there is a threshold, it determines the optimal threshold for each feature (i.e., the threshold that maximizes the expected information gain for that feature) and bases calculations on that threshold.
\end{itemize}

https://www.youtube.com/watch?v=7VeUPuFGJHk

\textbf{Classification Tree:} the output variable is discrete and finite (usually binary).

\textbf{Regression Tree:} the output variable has continuous values.

\subsection{Classification Tree}

\begin{itemize}
    \item Classification trees are recursive algorithms that split the dataset into smaller sets (nodes) in a step by step fashion thanks to binary rules defined on the features of the observations. 
    \item The complete dataset is the root node of the tree. To split the node, a feature is selected and a binary rule, e.g., if the value of the feature is larger or smaller than a given threshold, is defined on it in order to obtain two disjoint datasets. These two datasets become nodes of the tree. This procedure is repeated for each new node until a stopping criterion is met, e.g., the specified maximum depth of the tree is reached. 
    \item So a classification tree is a growing tree with nodes refining the information about the exogenous variables to classify an item in the database (in the binary case, class 0 or 1). 
    \item Each node is associated with a measure of \textit{Impurity}. Such a measure is high when the dataset representing the node contains observations belonging to different classes of the endogenous variable, and reaches its lowest value (zero) when the node only contains observations belonging to a single class.
    \item In the binary case, i.e. only two classes, we can use the Gini impurity. Given a node $x$, the Gini impurity, denoted by $G( x)$, for a binary classier is defined as
          \begin{equation*}
              G( x) =p( 1-p)
          \end{equation*}
          
          where $p$ denotes the proportion of observations belonging to class 1, contained in node $x$. $p$ can be interpreted as the probability of being of class 1 in the node. We can also use Entropy, instead of Gini index.
    \item We aim to minimize the Gini index; its maximum ($1/4$) corresponds to the case in which a node is perfectly balanced (50\% of the observations in the node are of class 0 and 50\% of class 1, and therefore $p=1/2$).
    \item The feature (exogenous variable) and the binary rule that are used for the split at a node are defined through a search process that maximizes the impurity decrease after the split. Given a split rule $r$ that divides node $x$ (observations of the dataset) into the nodes $x_{L}$ and $x_{R}$, we define the impurity decrease after the split as
          \begin{equation*}
              G\left( x\right) -p_{L} \cdot G\left( x_{L}\right) -p_{R} \cdot G\left( x_{R}\right)
          \end{equation*}
          
          where $p_{L}( p_{R})$ is the percentage of observations in $x$ that after the split belong to node $x_{L}( x_{R})$. The last nodes of the tree (the arrival point of the algorithm, i.e., the nodes that are not split anymore) are called \textit{leaves}. 
    \item The leaves contain the estimation of the tree (in our case class 0 or 1), which is given by the most recurrent class in that leaf: if more than 50\% of the observations in the leaf are of class 1, then the leaf is a class 1 leaf.
    \item We also get a probability of being of class 1 for each leaf, given by the proportion of observations of class 1 in that leaf.
\end{itemize}

\subsection{Regression Tree}

\begin{itemize}
    \item Regression trees have basically the same structure as classification trees, but the dependent variable is a continuous variable, and the measure of impurity is the MSE of the observations in the node. 
    \item The quantity predicted by each leaf is the average of the values of the target variable of the observations in the leaf.
    \item The MSE that measures the pureness of the node, computing the distance between the estimated value of the node (the average of the value of all the observations belonging to the node) and the value of the observations contained in the node, i.e., if there are $N$ observations belonging to a node, and pi is the value of observation i, $i=1,\dotsc ,N$, then we compute
          \begin{equation*}
              \frac{1}{N}\sum\limits _{i=1}^{N}\left( p_{i} -\frac{1}{N}\sum\limits _{j=1}^{N} p_{j}\right)^{2}
          \end{equation*}
    \item We construct a tree where instead of maximizing expected information gain we maximize the \textbf{expected decrease in mean squared error.}
\end{itemize}

\subsection{Random Forest}

\begin{itemize}
    \item This involves constructing many trees by for example:
          \begin{itemize}
              \item Using samples bootstrapped from the original data
              \item Using a random subset of features at each node
              \item Randomizing thresholds in some way
          \end{itemize}
    \item The final decision can be a majority vote or a weighted majority vote. Weights can reflect probability estimates (when available) or evidence from a hold-out test data set.
\end{itemize}

\textbf{Bagging}
\begin{itemize}
    \item Sample with replacement to create new data sets.
    \item Use voting or averaging methods for final estimate.
\end{itemize}

\textbf{Boosting}
\begin{itemize}
    \item Predictions are made sequentially, each trying to correct the previous error.
    \item One approach (AdaBoost) increases the weight given to misclassified observations.
    \item Another approach (Gradient boosting) tries to fit a new predictor to the error made by the previous predictor.
\end{itemize}

\textit{From Bagging...}
\begin{itemize}
    \item Bagging (Bootstrap Aggregation) is used to reduce the variance of classification/decision trees.
    \item The methodology builds several datasets from the training set choosing randomly observations with re-entry from the original dataset (statistical bootstrap).
    \item Each dataset is used to train a tree. As a result, we obtain an ensemble of trees-models and the final classifier is obtained averaging the predictions from the different trees yielding a more robust outcome than the one obtained from a single tree.
\end{itemize}

\textit{...to Random Forests}
\begin{itemize}
    \item A RF classifier is an extension of the bagging methodology. The approach considers random subsets of observations from the original dataset and it also randomly selects the features rather than using all the features to construct each tree.
    \item The output of the RF classifier is computed averaging the predictions of all the trees.
\end{itemize}

\begin{itemize}
    \item To calibrate the parameters of the RF we have to set a priori some hyper-parameters. The parameters of the model are then calibrated on the training set.
    \item The hyper-parameters are chosen evaluating the performance of the calibrated models on the validation set. 
          \begin{itemize}
              \item maximum tree depth: the maximum depth for a tree; 
              \item minimum leaf size: the minimum number of observations contained in a leaf. A split will only be considered if the number of observations belonging to each child node will be higher than the threshold;
              \item minimum decrease of impurity after a split: a node is split if it generates an impurity decrease greater than or equal to the threshold;
              \item minimum split size: the minimum number of observations belonging to a node required to split it;
              \item number of trees: the number of trees in the forest.
          \end{itemize}
\end{itemize}

\section{Supervised Learning: Neural Networks}

Perceptron

\fg{0.7}{perceptron}

A 3-layer Neural Network (NN) with 2 layers of hidden units

\fg{0.7}{87769a16d778f6cc934341c419b59760-Up0hDHH7BM-image.png}

Naming convention: a $N$\textbf{-layer} neural network with $N-1$ layers of \textbf{hidden units} and one \textbf{output layer}. Usually we don't count the input layer.

Usually every layer has its activation function.

\fg{0.7}{265707f6127a05ff50db85620d7e9ac7-1e25gtXUZr-image.png}

The most common activation functions are
\begin{equation*}
    \sigma ( z) =\frac{1}{1+e^{-z}} \ \ \ \ \tanh( z) =\frac{e^{z} -e^{-z}}{e^{z} +e^{-z}} \ \ \ \ \text{ReLU}( z) =\max( 0,z)
\end{equation*}

\fg{0.7}{c145a362119def1f36fc4b9e846ef6f7-0TDQNpDze4-image.png}

Remember that ReLU (Rectified Linear Unit) is unbounded.

\textit{Example.}

\fg{0.7}{5d1e4b9362a807b620c70f412a449166-zPMhvCQkFa-image.png}

Let us use the Sigmoid to relate the $v_{k}$ to the features.
\begin{equation*}
    v_{1} =\frac{1}{1+e^{-a_{1} -w_{11} x_{1} -w_{21} x_{2}}} \ \ \ \ v_{2} =\frac{1}{1+e^{-a_{2} -w_{12} x_{1} -w_{22} x_{2}}} \ \ \ \ v_{3} =\frac{1}{1+e^{-a_{3} -w_{13} x_{1} -w_{23} x_{2}}}
\end{equation*}
The output is a bias (constant) plus linear combination of the preceding values
\begin{equation*}
    H=c+u_{1} v_{1} +u_{2} v_{2} +u_{3} v_{3}
\end{equation*}
We have $13$ degrees of freedom.

\textbf{Universal Approximation Theorem. }Any continuous function can be approximated to arbitrary accuracy with one hidden layer\footnote{See: K. Hornik, Neural Networks, 1991, 4:251-257.}, but this may require a very large number of neurons. Using several hidden layers can be computationally more efficient.

However, the theorem doesn't tell how many neurons or layers.

If there are $F$ features, $H$ hidden layers, $M$ neurons in each hidden layer and $T$ targets the number of parameters is:
\begin{equation*}
    \text{\#parameters} =( F+1) M+M( M+1)( H-1) +( M+1) T
\end{equation*}

\textbf{How to deal with such a large number of parameters?}

To minimize an objective function such as MSE, a \textbf{gradient descent} algorithm calculates the direction of steepest descent, takes a step, calculate a new direction of steepest descent, takes another step, and so on. The partial derivatives with respect to the parameters are calculated by a procedure known as \textbf{backpropagation}.

The size of the step is the \textbf{learning rate:} if the step is too small the algorithm will be very slow. If it is too large there are liable to be oscillations.

\textbf{Stopping rule}

It is important to use a stopping rule to \textbf{avoid over-fitting}. We calculate results for both the validation set and the training set: \textbf{when the results for the validation set start to get worse we stop}.

\subsection{Convolutional Neural Network (CNN)}

In a vanilla ANN the layers are \textbf{fully connected} which can give rise to a very large number of parameters. In a CNN, the number of neurons in one layer that affect the next layer is \textbf{reduced}. Often used for image recognition where each pixel can be a feature.

\textbf{Key advantages.} Consider a image that is $100 \cdot 100$ or $10.000$ pixels. A regular neural network would lead to $10.000\cdot(10.000+1)$ or about $100$ million parameters to define the first layer. If a CNN's receptive field is $10\cdot 10$ and a layer has $6$ feature maps only $6\cdot 101 = 606$ are required. More importantly, once the CNN has learned to recognize a pattern in one location it can recognize it in another location.


\subsection{Recurrent Neural Network (RNN)}

In a vanilla ANN, the $v$'s are functions of the inputs and the sequence of the inputs does not matter. In a recurrent neural network there is a \textbf{time dimension} in the data. The current $v$'s are made functions of the corresponding previous $v$'s (or possibly previous outputs) as well as current inputs. A \textbf{long short-term memory (LSTM)} network is a type of RNN where part of the algorithm learns what should be remembered and what should be forgotten from previous data.

Applications are when working with sequential data where the relationship between targets and features may be changing through time. Also used in NLP (autocompletion for example).

% \textbf{Black-Scholes-Merton Application}

% Goal Option

% Let $S_{t}$ be a \textbf{stock} at time $t$. You can also buy \textbf{Financial Derivatives} (like \textbf{European Call Options}, something which have a \textbf{maturity} $T$ (years), a \textbf{strike} $K$ and a \textbf{payoff} $( S_{T} -K)^{T}$).

% In the B\&S\&M framework we can model
% \begin{equation*}
%     [ \dotsc unreadable\ from\ blackboard\dotsc ]
% \end{equation*}
% which is highly non-linear. We can use NN to learn this formula.

\section{Interpretability}

\textbf{Why is model interpretability important?}
\begin{itemize}
    \item Users must understand a model to have confidence in it, know when it is appropriate, be aware of its biases, etc
    \item It is also important to be able to explain the predictions made by the model, e.g.,
          \begin{itemize}
              \item Why was someone refused for a loan?
              \item Why is house A worth more than house B
          \end{itemize}
    \item The General Data Protection Regulation in the European Union requires model interpretability
\end{itemize}

\textbf{White-box vs black-box models}

White-box models
\begin{itemize}
    \item $k$-nearest neighbors
    \item Decision trees
    \item Linear regression
\end{itemize}

Black-box models
\begin{itemize}
    \item Neural networks
    \item Ensemble models (e.g. random forests)
\end{itemize}

\textit{Interpretability issue: explain black-box algorithm decisions!}

Features dependency could be a problem: we might be able to group features that should be considered together. Sometimes a PCA is used to create uncorrelated features. 

\textbf{Black-box models}
\begin{itemize}
    \item Models must be re-run to determine \textit{the impact of the change in a feature value on a prediction.}
    \item In general there is non-linearity so that when changes are made to the feature values the sum of the contributions of the features does not equal the change in the prediction
\end{itemize}

\textbf{Local Interpretability:} why the black-box model returns a prediction for a given observation?

\textbf{Global Interpretability:} what are the drivers of the black-box predictions?

\subsection{LIME}

LIME tries to understand a black-box model by fitting a simpler model to data that is close to the currently observed data (Local Interpretability).

Procedure is:
\begin{enumerate}
    \item Perturb feature values to get a samples
    \item Run black-box model to get predictions for samples
    \item Train an easy to interpret model such as linear regression or decision trees to fit the data set that is created from samples and predictions
\end{enumerate}

Global Interpretability as LIME approach:

\subsection{Partial Dependence Plot}

The partial dependence plot is the expected prediction as a function of the value of a particular feature.

The values of all features except the one under consideration are chosen randomly 

\subsection{Shapley Values}

\begin{itemize}
    \item The SHAP (SHapley Additive exPlanations) method allows us to capture the impact of the different features on the ML output. 
    \item The method borrows from cooperative game theory and consists in the calculation of SHAP value, which represents a measure of the importance of a feature. 
    \item More precisely, the SHAP value of a feature measures how much it contributes, either positively or negatively, to the classifier prediction.
    \item The goal of the SHAP method is to explain a prediction computing the contribution of each feature to the prediction itself.
    \item More precisely, the method shows the contribution of each feature to push the model output from the base value (the average model output over the training dataset) to the model output associated with the observation. Given a single observation, a set of SHAP values, one for each feature, is calculated.
\end{itemize}

\textbf{Properties}
\begin{itemize}
    \item If a feature never changes the prediction, its contribution is zero. 
    \item If two features are symmetrical in that they affect the prediction in the same way, they have the same contribution. 
    \item For an ensemble model where predictions are the average of predictions given by several underlying models, the Shapley value is the average of the Shapley values for the underlying models. 
    \item Calculation time increases exponentially with the number of features
\end{itemize}

\subsection{Global Interpretability in Random Forests}

\paragraph{Mean Decrease Accuracy (\%IncMSE)}
This shows how much our model accuracy decreases if we leave out \textit{\textbf{that feature.}}

\paragraph{Mean Decrease Gini (IncNodePurity)}
This is the total decrease in node impurities, measured by the Gini Index from splitting on \textit{\textbf{the feature}}, averaged over all trees.

The higher the value of mean decrease accuracy or mean decrease Gini score, the higher the importance of the feature to our model.

\section{Reinforcement Learning}

Reinforcement learning is concerned with finding a strategy for taking a series of decisions rather than just one.
The environment is usually changing unpredictably.

\subsection{Rewards and costs}

\begin{itemize}
    \item There are rewards and costs and the algorithm tries to maximize expected rewards (net of costs) in its interaction with the environment 
    \item Exploitation vs. Exploration (Should you choose best decision based on evidence to date or try something new?)
    \item Action evaluated by the sum of expected rewards (net of costs) that come after it (possibly discounted)
\end{itemize}

Rewards The time horizon may be
\begin{itemize}
    \item finite: finite and fixed number of steps
    \item indefinite: until some stopping criteria is met (\emph{absorbing} states)
    \item infinite: forever
\end{itemize}
There are different number of rewards.
\begin{itemize}
    \item \textbf{Total reward}
    \[
        V = \sum_{i=1}^{\infty} r_{i} 
    \]
    \item \textbf{Average reward}
    \[
        V = \lim_{n \to \infty} \frac{r_{1} + \cdots + r_{n}}{n}
    \]
    \item \textbf{Discounted reward}
    \[
        V = \sum_{i=1}^{\infty} \gamma ^{i-1} r_{i}  
    \]
\end{itemize}

We define the \textbf{return} $v_{t}$ as the total discounted reward from time $t+1,\ldots$
\[
    v_{t} = r_{t+1} + \gamma r_{t+2} + \cdots = \sum_{k=0}^{\infty} \gamma ^{k} r_{t+k+1}   
\]
\begin{itemize}
    \item The \textbf{discount} $\gamma \in [0,1)$ is the present value of future rewards.
    \item The value of receiving reward $r$ after $k+1$ steps is $\gamma ^{k}r $
    \item \textbf{Immediate} reward vs \textbf{delayed} reward
    \begin{itemize}
        \item $\gamma $ close to $0$ leads to myopic evaluation
        \item $\gamma $ close to $1$ leads to far-sighted evaluation
    \end{itemize}
    \item $\gamma $ can also be interpreted as the probability that the process will go on.
\end{itemize}

\subsection{Policies}

A policy, at any given point in time, \textbf{decides} which action the agent selects. A policy fully defines the \textbf{behavior} of an agent.

\paragraph{Value function} Given a policy $\pi $, it is possible to define the \textbf{utility} of each state: \textbf{Policy Evaluation}. The state-value function $V^{\pi }(s) $ of an MDP (Markov Decision Process) is the expected return starting from state $s$, and then following policy $\pi $
\[
    V^{\pi }(s) = \EE_{\pi }[v_{t}\mid s_{t}=s] 
\]
For control purposes, rather than the value of each state, it is easier to consider the value of each action in each state. The action-value function $Q^{\pi }(s,a) $ is the expected return starting from state $s$, taking action $a$, and then following policy $\pi $
\[
    Q^{\pi }(s,a) = \EE_{\pi }[v_{t} \mid s_{t}=s,a_{t}=a] 
\]

\paragraph{Optimal value function} The \textbf{optimal state-value function} $V^{\star}(s) $ is the maximum value function over all policies
\[
    V^{\star}(s) = \max_{\pi }V^{\pi }(s)  
\]
The \textbf{optimal action-value function} $Q^{\star}(s,a) $ is the maximum action-value function over all policies
\[
    Q^{\star}(s,a) = \max_{\pi }Q^{\pi }(s,a)
\]
The optimal value function specifies the best possible performance in the MDP. An MDP is \emph{solved} when we know the optimal value function.

\paragraph{Optimal Policy} Value functions define a partial ordering over policies
\[
    \pi \geq \pi ' \iff V^{\pi }(s)\geq V^{\pi'}(s), \forall s\in \Sc
\]
Theorem: for any Markov Decision Process
\begin{itemize}
    \item There exists an \textbf{optimal policy} $\pi ^{\star} $ that is better than or equal to all policies $\pi ^{\star}\geq \pi ,\forall \pi  $
    \item \textbf{All} optimal policies achieve the optimal value function, $V^{\pi ^{\star} }(s) = V^{\star}(s)$
    \item \textbf{All} optimal policies achieve the optimal action-value function, $Q^{\pi ^{\star} }(s,a) = Q^{\star}(s,a)$
    \item There is always a \textbf{deterministic optimal policy} for any MDP.
\end{itemize}
A deterministic optimal policy can be found by maximizing over $Q^{\star}(s,a) $
\[
    \pi ^{\star}(a\mid s) = \begin{cases}
        1 & \text{if } a = \argmax_{a\in \Ac} Q^{\star}(s,a) \\
        0 & \text{otherwise}
    \end{cases} 
\]

\paragraph{Monte-Carlo Policy Evaluation}
\begin{itemize}
    \item \textbf{Goal}: learn $V^{\pi}$ from experience under policy $\pi$
    \[
    s_{1}, a_{1}, r_{2}, \ldots, s_{T} \sim \pi
    \]
    \item Recall that the \textbf{return} is the total discounted reward:
    \[
    v_{t}=r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^{T-1} r_{t+T}
    \]
    \item Recall that the \textbf{value function} is the expected return:
    \[
    V^{\pi}(s)=\mathbb{E}\left[v_{t} \mid s_{t}=s\right]
    \]
    \item Monte Carlo policy evaluation uses \textbf{empirical mean} return instead of expected return
    \begin{itemize}
        \item \textbf{first visit}: average returns only for the first time $s$ is visited (\textbf{unbiased} estimator)
        \item \textbf{every visit}: average returns for every time $s$ is visited (\textbf{biased but consistent estimator})
    \end{itemize}
\end{itemize}

\paragraph{TD Prediction}
\begin{itemize}
    \item \textbf{Goal}: learn $V^{\pi}$ online from experience under policy $\pi$
    \item \textbf{Recall}: incremental every-visit Monte Carlo
    \[
    V\left(s_{t}\right)=V^{\text {new }}\left(s_{t}\right)+\alpha\left(v_{t}-V^{\text {old }}\left(s_{t}\right)\right)
    \]
    \item \textbf{Simplest} temporal-difference learning algorithm: $\mathrm{TD}(0)$
    \begin{itemize}
        \item Update value $V\left(s_{t}\right)$ towards estimated return $r_{t+1}+\gamma V\left(s_{t+1}\right)$
        \[
        V\left(s_{t}\right)=V^{\text {new }}\left(s_{t}\right)+\alpha\left(r_{t+1}+\gamma V^{\text {old }}\left(s_{t+1}\right)-V^{\text {old }}\left(s_{t}\right)\right)
        \]
        \item $r_{t+1}+\gamma V\left(s_{t+1}\right)$ is called the TD target
        \item $\delta_{t}=r_{t+1}+\gamma V\left(s_{t+1}\right)-V\left(s_{t}\right)$ is called the TD error
    \end{itemize}
\end{itemize}

We can update either $V$ or $Q$

\begin{itemize}
    \item MC: we usually set
    or
    \[
    Q(s, a)^{\text{new}}=Q(s, a)^{\text{old}}+\frac{1}{n}\left[v-Q(s, a)^{\text{old}}\right] \text{ or } Q(s, a)^{\text{new}}=Q(s, a)^{\text{old}}+\alpha\left[v-Q(s, a)^{\text{old}}\right]
    \]
    for some parameter $\alpha$ (e.a. $0.1)$
    \item TD:
\[
Q(s, a)^{\text{new}}=Q(s, a)^{\text{old}}+\alpha\left[r+\gamma V^{\text{old}}-Q(s, a)^{\text{old}}\right]
\]
\end{itemize}

\paragraph{On-policy exploration}
\begin{itemize}
    \item There are two doors in front of you
    \item You open the left door and get reward 0 , $V(\text{left})=0$
    \item You open the right door and get reward $+1$, $V(\text{right})=+1$
    \item You open the right door and get reward $+3$, $V(\text{right})=+2$
    \item You open the right door and get reward $+2$, $V(\text{right})=+2$
\end{itemize}

Are you sure you've chosen the best door?

\paragraph{$\varepsilon$-Greedy exploration}

\begin{itemize}
    \item Simplest idea for ensuring \textbf{continual exploration}
    \item \textbf{All} $m$ actions are tried with \textbf{non-zero} probability
    \item With probability $1-\varepsilon$ choose the \textbf{greedy} action
    \item With probability $\varepsilon$ choose an action at \textbf{random}
    \[
    \pi(s, a)= \begin{cases}\frac{\varepsilon}{m}+1-\varepsilon & \text {if } a^{\star}=\argmax _{a \in \Ac} Q(s, a) \\ \frac{\varepsilon}{m} & \text {otherwise}\end{cases}
    \]
\end{itemize}


\paragraph{MC vs TD Control}
\begin{itemize}
    \item Temporal-Difference (TD) learning has several advantages over Monte-Carlo (MC)
    \begin{itemize}
        \item Lower Variance
        \item Online
        \item Incomplete sequences
    \end{itemize}
    \item Natural idea: use TD instead of MC in our control loop
    \begin{itemize}
        \item Apply TD to $Q(s, a)$
        \item Use $\varepsilon$-Greedy policy improvement
        \item Update every time-step
    \end{itemize}
\end{itemize}

The exploration parameter $\varepsilon$ and initial values. It makes sense to reduce $\varepsilon$ over time. For example we can let it decline exponentially
The initial $Q$-values makes a difference.
If the standard deviation of the payoff is increased, a higher value of $\varepsilon$ would be appropriate.

\paragraph{$n$-step bootstrapping} Monte Carlo bases updates on what happens over the complete life of the trial.
Temporal difference bases updates on what happens over the next period .
\textbf{$n$-step bootstrapping algorithms} is between the two. It bases updates on what happens over the next $n$ periods.

\paragraph{When there are many states or actions (or both)}

\begin{itemize}
    \item The cells of the state/action table do not get filled in very quickly
    \item It becomes necessary to estimate the $Q(s,a)$ function from observed values.
    \item As this function is in general non-linear a natural approach is to use artificial neural networks (ANNs) or RFs (as in the Forex example in the next slide).
    \item We use an ANN or a RF to minimize the sum of squared errors between the estimates and the target
    \item This is known as deep $Q$-learning or deep reinforcement learning.
\end{itemize}
